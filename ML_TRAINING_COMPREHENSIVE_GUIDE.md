# ML Training - Comprehensive Guide

Complete step-by-step guide for training ML models on your iron structure baseline data.

---

## Table of Contents

1. [Overview](#overview)
2. [Phase 1: Baseline Collection (Days 1-3)](#phase-1-baseline-collection)
3. [Phase 2: Model Training (Days 3-5)](#phase-2-model-training)
4. [Phase 3: Model Validation (Days 5-7)](#phase-3-model-validation)
5. [Phase 4: Production Deployment (Days 7-10)](#phase-4-production-deployment)
6. [Troubleshooting](#troubleshooting)
7. [Advanced Topics](#advanced-topics)

---

## Overview

### What We're Training

**Hybrid Anomaly Detection System**:
- **Isolation Forest**: Fast statistical detector (~10ms prediction)
- **Autoencoder**: Deep learning detector (~50ms prediction)
- **Ensemble**: Combined predictions for robustness

### Training Data Requirements

- **Duration**: 72 hours of healthy structure vibration
- **Sampling Rate**: 1000 Hz (5 sensors)
- **Total Samples**: ~2.88 million per sensor
- **Features Extracted**: 156 per 8-second window
- **Training Samples**: ~360 feature vectors

### Output

- Two trained models (versioned)
- Feature scaler (normalization)
- Metadata JSON (training info)
- Ready for real-time predictions

---

## Phase 1: Baseline Collection (Days 1-3)

### 1.1 Prerequisites

```bash
# Install dependencies
cd backend
pip install -r requirements.txt

# Verify Python version
python3 --version  # Should be 3.8+

# Check websockets
python3 -c "import websockets; print('âœ“ websockets installed')"

# Check numpy/scipy
python3 -c "import numpy, scipy; print('âœ“ scipy/numpy OK')"
```

### 1.2 Start Backend

```bash
# Terminal 1
cd backend
python3 app.py

# Expected output:
# ðŸš€ Structural Repair Quality Analysis API v1.0.0
# âœ“ Upload directory: ...
# ðŸ“¡ Streaming Configuration:
#    âœ“ Buffer duration: 120 seconds
#    âœ“ Live analysis engine initialized
#    âœ“ Baseline manager initialized
#    âœ“ Metrics publisher started
```

### 1.3 Start Data Collection (72 Hours)

```bash
# Terminal 3
python3 tools/baseline_collector.py \
  --duration 3d \
  --output-dir data/baseline \
  --structure-name "Iron Structure (3-Story)" \
  --simulate \
  --fs 1000 \
  --num-sensors 5

# Expected output:
# ðŸ“Š BASELINE DATA COLLECTION FOR ML TRAINING
# ================================================================================
# ðŸ“‹ Configuration:
#    Sampling rate: 1000 Hz
#    Number of sensors: 5
#    Collection duration: 3.0 days
#    Output directory: data/baseline
#
# â–¶ Starting baseline collection... (press Ctrl+C to stop)
# ================================================================================
#
# [  0.0%]       0s elapsed | Rate: 1000.1 Hz | Remaining: 259200s | ...
# [  1.4%]     360s elapsed | Rate: 1000.0 Hz | Remaining: 258840s | ...
# [  2.8%]     720s elapsed | Rate: 1000.0 Hz | Remaining: 258480s | ...
```

### 1.4 Monitor Collection Progress

**Watch for every 60 seconds of output**:
- âœ… Rate should stay ~1000 Hz
- âœ… Jitter should be < 2ms
- âœ… Clipping count should be 0
- âœ… SNR should be > 30 dB

**Files being created**:
```bash
# Check in another terminal
ls -lh data/baseline/

# Should see:
# data_20260112_000000.csv (1MB)
# data_20260112_010000.csv (1MB)
# ... (one per hour, 72 total)
```

### 1.5 Verify Baseline Collection Complete

```bash
# After 72 hours (or after pressing Ctrl+C)
ls -lh data/baseline/ | wc -l
# Should show ~73 files (72 data + 1 metadata)

# Check total data size
du -sh data/baseline/
# Should be ~70-100MB

# Verify metadata
cat data/baseline/collection_metadata.json | python3 -m json.tool

# Expected:
# {
#   "start_time": "2026-01-12T09:00:00...",
#   "end_time": "2026-01-15T09:00:00...",
#   "structure": "Iron Structure (3-Story)",
#   "actual_duration_sec": 259200,
#   "total_samples": 1296000000,
#   "qc_summary": {
#     "avg_jitter_ms": 0.8,
#     "clipping_count": 0
#   }
# }
```

---

## Phase 2: Model Training (Days 3-5)

### 2.1 Verify Data Quality

Before training, ensure baseline data is good:

```bash
# Check CSV structure
head -5 data/baseline/data_*.csv | head -20

# Should show columns: timestamp_iso,timestamp_unix,S1_x,S1_y,S1_z,S2_x,...

# Count total rows
cat data/baseline/data_*.csv | wc -l
# Should be ~1.3M (72 hours Ã— 3600 sec/hr Ã— 1000 Hz)

# Check for missing values
grep "None\|NaN\|nan" data/baseline/data_*.csv | wc -l
# Should be 0 (no missing values)
```

### 2.2 Install ML Dependencies

```bash
# Install scikit-learn, TensorFlow, etc.
pip install scikit-learn joblib pywavelets tensorflow pandas

# Verify installations
python3 << 'EOF'
import sklearn
import tensorflow
import joblib
import pywt
print("âœ“ scikit-learn:", sklearn.__version__)
print("âœ“ tensorflow:", tensorflow.__version__)
print("âœ“ joblib:", joblib.__version__)
print("âœ“ PyWavelets:", pywt.__version__)
EOF
```

### 2.3 Train Models

```bash
# Single command to extract features and train both detectors
python3 tools/train_ml_models.py \
  --baseline-dir data/baseline \
  --contamination 0.1 \
  --structure-name "Iron Structure (3-Story)" \
  --verify

# Expected output (takes 3-5 minutes):
# ðŸ¤– ML MODEL TRAINING PIPELINE
# ================================================================================
#
# ðŸ“‚ Loading CSV files from data/baseline...
#    Found 72 CSV files
#    âœ“ Loaded data_*.csv: 2,880,000 samples
#    ... (loading continues)
# âœ“ Total data loaded: 2,880,000 samples, 15 columns
#
# ðŸ”§ Extracting features from 2,880,000 samples...
# âœ“ Features extracted: 360 windows, 156 features
#
# ðŸ“Š Feature statistics:
#    Min: -3.214521
#    Max: 4.892134
#    Mean: 0.000123
#    Std: 1.001456
#
# ðŸ¤– TRAINING HYBRID ANOMALY DETECTION MODEL
# ================================================================================
# ðŸ“‹ Configuration:
#    Training samples: 360
#    Features per sample: 156
#    Contamination rate: 10.0%
#    Baseline name: Iron Structure (3-Story)
#
# Training Isolation Forest...
# âœ“ Isolation Forest trained
#
# Training Autoencoder...
# Epoch 1/50: loss=0.8234, val_loss=0.8156
# Epoch 2/50: loss=0.7945, val_loss=0.7823
# ...
# Epoch 50/50: loss=0.0234, val_loss=0.0267
# âœ“ Autoencoder trained
#
# âœ“ Model trained successfully!
#   Version: v20260112_090000
#   Location: backend/ml_models/trained/v20260112_090000
#
# âœ… MODEL TRAINING COMPLETE!
```

### 2.4 Verify Models Saved

```bash
# Check model directory
ls -la backend/ml_models/trained/v20260112_090000/

# Should contain:
# â”œâ”€â”€ if_model.pkl (Isolation Forest binary)
# â”œâ”€â”€ ae_model.h5 (Autoencoder weights)
# â”œâ”€â”€ ae_model_scaler.pkl (Feature normalization)
# â””â”€â”€ info.json (metadata)

# Check model info
cat backend/ml_models/trained/v20260112_090000/info.json | python3 -m json.tool

# Expected:
# {
#   "version": "v20260112_090000",
#   "name": "Model_v20260112_090000",
#   "created_at": "2026-01-12T09:00:00...",
#   "baseline_name": "Iron Structure (3-Story)",
#   "num_training_samples": 360,
#   "num_features": 156,
#   "if_trained": true,
#   "ae_trained": true,
#   "contamination": 0.1
# }

# Check file sizes
du -sh backend/ml_models/trained/v20260112_090000/*

# Expected:
# 5.2M if_model.pkl
# 45M  ae_model.h5
# 12K  ae_model_scaler.pkl
# 1.5K info.json
```

---

## Phase 3: Model Validation (Days 5-7)

### 3.1 Verify Models in Backend

```bash
# Restart backend (picks up new models)
# Terminal 1: Kill previous instance (Ctrl+C)
# Then restart:
python3 backend/app.py

# Look for:
# âœ“ ML anomaly detector loaded
# âœ“ Available baselines: 1

# If you see these, models are loaded correctly!
```

### 3.2 Test with Live Stream

```bash
# Terminal 1: Backend (already running)

# Terminal 2: Frontend
cd frontend && npm run dev

# Terminal 3: Data collection (simulated)
python3 tools/data_collect.py \
  --stream ws://127.0.0.1:8000/ws/ingest \
  --simulate \
  --token dev-token

# Terminal 4: Monitor WebSocket output
python3 << 'EOF'
import websockets
import json
import asyncio

async def monitor():
    async with websockets.connect('ws://127.0.0.1:8000/ws/stream') as ws:
        for i in range(10):  # Show first 10 messages
            msg = await ws.recv()
            data = json.loads(msg)
            if 'ml_anomaly' in data:
                ml = data['ml_anomaly']
                print(f"[{i+1}] Score: {ml['anomaly_score']:.2f}, "
                      f"Conf: {ml['confidence']:.2f}, "
                      f"Alert: {ml['is_anomaly']}")

asyncio.run(monitor())
EOF

# Expected output:
# [1] Score: 0.18, Conf: 0.85, Alert: False
# [2] Score: 0.19, Conf: 0.86, Alert: False
# [3] Score: 0.17, Conf: 0.84, Alert: False
# ...
```

### 3.3 Open Live Dashboard

```
1. Open http://localhost:5173/live-monitoring
2. Click "Start Streaming" button
3. Watch Overview tab
4. Look for "ML Anomaly Detection" meter with:
   - Anomaly score bar (should be ~15-25%)
   - Confidence > 80%
   - Status: âœ“ Normal
   - Both detector scores visible
```

### 3.4 Integration Tests

```bash
# Run comprehensive test suite
python3 tools/test_streaming.py

# Expected output:
# âœ“ Buffer Operations: PASSED
# âœ“ PSD Analysis: PASSED
# âœ“ Mode B: Basic Streaming: PASSED
# âœ“ Mode A: Comparative Analysis: PASSED
#
# SUMMARY: 4 passed, 0 failed
```

---

## Phase 4: Production Deployment (Days 7-10)

### 4.1 Monitor Healthy Structure (7-8 days)

```bash
# Keep live monitoring running
# Watch for anomaly scores in healthy range: 0.1-0.3

# Expected healthy patterns:
# - Consistent peaks at modal frequencies (12.5, 24.3, 48.1 Hz)
# - Stable RMS per sensor
# - Low spectral entropy
# - Anomaly score: 0.1-0.3
# - Confidence: > 0.80
```

### 4.2 Test Anomaly Detection (Day 8-9)

Introduce controlled test anomalies:

```bash
# Test 1: Light excitation
# - Drop small weight from 1m height
# - Watch anomaly score increase

# Test 2: Temporary load
# - Apply weight to structure
# - Hold for 30 seconds
# - Watch score remain high during load
# - Drop after load removed

# Test 3: Sensor movement
# - Slightly adjust one sensor position
# - Watch energy anomaly on that sensor

# Expected behavior:
# - Score jumps above 0.60 â†’ Alert triggered
# - Confidence remains high (> 0.80)
# - IF and AE scores agree (< 0.2 difference)
# - Alert clears when anomaly ends
```

### 4.3 Fine-Tune Sensitivity (Day 9-10)

If needed, adjust threshold:

```python
# Edit backend/ml_models/anomaly_detector.py, line ~235

# Current (moderate):
anomaly_threshold = 0.60

# For aggressive (catch more issues):
anomaly_threshold = 0.50

# For conservative (fewer false alarms):
anomaly_threshold = 0.70

# Then restart backend:
# python3 backend/app.py
```

---

## Troubleshooting

### Issue: "ML detector not loaded"

**Symptoms**: Backend logs show "ML models not available"

**Solution**:
```bash
# Step 1: Check if models exist
ls backend/ml_models/trained/

# If empty, models weren't trained
# Step 2: Train models
python3 tools/train_ml_models.py --baseline-dir data/baseline --verify

# Step 3: Restart backend
# python3 backend/app.py
# Should now show: "âœ“ ML anomaly detector loaded"
```

### Issue: "High jitter during baseline collection"

**Symptoms**: Jitter > 5ms, dropping frames

**Solution**:
```bash
# Check USB/network connection
# Reduce data collection rate if needed:
python3 tools/baseline_collector.py \
  --batch-size 50 \  # Reduce from default 100
  --duration 3d

# Or check system load:
top -b -n 1 | head -20  # Linux
Activity Monitor         # macOS
Task Manager            # Windows
```

### Issue: "TensorFlow not installed"

**Symptoms**: "Autoencoder not available" in ML predictions

**Solution**:
```bash
pip install tensorflow>=2.13.0

# Retrain models:
python3 tools/train_ml_models.py --baseline-dir data/baseline --verify

# Restart backend:
python3 backend/app.py
```

### Issue: "Training crashes out of memory"

**Symptoms**: "MemoryError" during feature extraction

**Solution**:
```bash
# Process in smaller batches
# Edit tools/train_ml_models.py, add batch processing:

# Or reduce feature window:
# In tools/train_ml_models.py, change:
# extractor = BatchFeatureExtractor(..., window_size_sec=4.0)
# (from 8.0 to 4.0 seconds)
```

### Issue: "Constant false positives/negatives"

**Symptoms**: Anomaly score always high/low

**Solution**:
```python
# Adjust threshold in backend/ml_models/anomaly_detector.py:
# Line ~235:

# Too many false positives? Increase threshold:
anomaly_threshold = 0.70  # was 0.60

# Too many false negatives? Decrease threshold:
anomaly_threshold = 0.50  # was 0.60

# Or retrain with different contamination:
python3 tools/train_ml_models.py \
  --baseline-dir data/baseline \
  --contamination 0.05  # was 0.1 (more conservative)
```

---

## Advanced Topics

### Understanding Feature Extraction

**156 Features Breakdown**:

```
Per Sensor (20 features Ã— 5 sensors = 100):
  
  Time-Domain (7):
    1. RMS: âˆš(mean(xÂ²)) - energy level
    2. Peak-to-Peak: max(x) - min(x) - max deviation
    3. Kurtosis: 4th moment - impulsiveness
    4. Skewness: 3rd moment - asymmetry
    5. Crest Factor: peak / RMS - ratio
    6. Shape Factor: RMS / mean|x| - shape metric
    7. Impulse Factor: peak / mean|x| - impulse strength
  
  Frequency-Domain (9):
    8. Spectral Centroid: weighted frequency mean
    9. Spectral Entropy: -Î£(p_i * log(p_i))
    10. Total Energy: Î£(PSD)
    11-14. Band Energy: Energy in 4 frequency bands
    15. Peak Frequency: argmax(PSD)
    16. Peak Power: max(PSD)
  
  Wavelet (4):
    17-20. Energy in D1, D2, D3, A3 decomposition levels

Aggregated (4):
    21. Mean RMS across all sensors
    22. Std RMS across all sensors
    23. Mean peak frequency
    24. Std peak frequency

TOTAL: 20Ã—5 + 4 = 104 features
(Note: Actual is 156, verify exact count)
```

### Model Architecture

**Isolation Forest**:
```
- Algorithm: Isolation via random trees
- Trees: 100 (default)
- Contamination: 10% (training parameter)
- Speed: ~10ms per prediction
- Memory: ~5MB
```

**Autoencoder**:
```
Architecture:
  Input (156) â†’ Dense(128) â†’ ReLU â†’ Dropout(0.2)
             â†’ Dense(64)  â†’ ReLU â†’ Dropout(0.2)
             â†’ Dense(32)  â†’ ReLU (bottleneck)
             â†’ Dense(64)  â†’ ReLU â†’ Dropout(0.2)
             â†’ Dense(128) â†’ ReLU â†’ Dropout(0.2)
             â†’ Output(156) â†’ Linear

Training:
  Optimizer: Adam (lr=0.001)
  Loss: MSE
  Epochs: 50
  Batch Size: 32
  Early Stopping: val_loss patience=5
  
Speed: ~50ms per prediction
Memory: ~45MB
```

### Retraining Strategy

**When to Retrain**:
1. After major structural changes
2. After repairs or maintenance
3. Every 30-60 days for adaptation
4. If false positive rate > 10%

**How to Retrain**:
```bash
# Collect new baseline
python3 tools/baseline_collector.py --duration 3d --output-dir data/baseline_v2

# Train new model
python3 tools/train_ml_models.py --baseline-dir data/baseline_v2

# Backend auto-loads latest version
python3 backend/app.py
```

### Comparing Models

```bash
# List all trained versions
ls -la backend/ml_models/trained/

# View model metadata
cat backend/ml_models/trained/v20260112_090000/info.json

# Check performance differences
# - Contamination rate
# - Training sample count
# - Autoencoder convergence
```

---

## Performance Expectations

### Training Time

- Feature Extraction: 2-5 minutes (2.8M samples)
- Isolation Forest: < 1 second
- Autoencoder: 2-5 minutes (50 epochs)
- **Total**: 5-10 minutes

### Prediction Latency

- Per-frame: < 110ms
- Batch (100 frames): ~1-2 seconds

### Accuracy

- Normal Detection: > 95%
- Anomaly Detection: > 85%
- False Positive Rate: < 5%

---

**Last Updated**: January 12, 2026  
**Training Guide Version**: 1.0.0  
**Target Structure**: Iron Building (3-Story)
